{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = np.load('data/GOGO/trainingdata/Play_style/organizeddata/NPZ/PS_train_dataset3_feature10_v3.npz')\n",
    "\n",
    "# print(loaded_data.files)\n",
    "\n",
    "x_train = loaded_data['x_train']\n",
    "y_train = loaded_data['y_train']\n",
    "\n",
    "x_train.shape ,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y,C):\n",
    "  Y = np.eye(C)[Y.reshape(-1)].T\n",
    "  return Y\n",
    "\n",
    "\n",
    "train_labels = convert_to_one_hot(y_train-1,3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, train_labels, test_size=0.2, random_state=1123,stratify = train_labels)\n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_images_and_labels(images, labels):\n",
    "    rotated_images = []\n",
    "    rotated_labels = []\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        rotated_90 = np.rot90(image, k=1, axes=(0, 1))\n",
    "        rotated_180 = np.rot90(image, k=2, axes=(0, 1))\n",
    "        rotated_270 = np.rot90(image, k=3, axes=(0, 1))\n",
    "        \n",
    "        rotated_images.extend([image, rotated_90, rotated_180, rotated_270])\n",
    "        rotated_labels.extend([labels[i], labels[i], labels[i], labels[i]])\n",
    "        \n",
    "    return np.array(rotated_images), np.array(rotated_labels)\n",
    "\n",
    "rotated_train_images, rotated_train_labels = rotate_images_and_labels(x_train, y_train)\n",
    "rotated_train_images.shape, rotated_train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, LayerNormalization,Softmax,Dropout,BatchNormalization ,Concatenate,Reshape,GlobalMaxPooling2D ,Add,Activation  ,multiply,Lambda,GlobalAveragePooling2D\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    cdf = 0.5 * (1.0 + tf.tanh(\n",
    "        (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block(x):\n",
    "    x1 = Conv2D(16, (1, 1), activation='relu', padding='same')(x)\n",
    "    \n",
    "    x3 = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x3 = Conv2D(192, (3, 3), activation='relu', padding='same')(x3)\n",
    "    \n",
    "    x9 = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x9 = Conv2D(128, (9, 9), activation='relu', padding='same')(x9)\n",
    "    \n",
    "    concatted  = Concatenate()([x1,x3,x9]) #,x5,x7\n",
    "    \n",
    "    concatted =  LayerNormalization()(concatted)\n",
    "    concatted = gelu(concatted)\n",
    "    return concatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_attention(input_feature, ratio=8):\n",
    "\t\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "\t\n",
    "\tshared_layer_one = Dense(channel//ratio,\n",
    "\t\t\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tshared_layer_two = Dense(channel,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\t\n",
    "\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "\tavg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tavg_pool = shared_layer_one(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tavg_pool = shared_layer_two(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\t\n",
    "\tmax_pool = GlobalMaxPooling2D()(input_feature)\n",
    "\tmax_pool = Reshape((1,1,channel))(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = shared_layer_one(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tmax_pool = shared_layer_two(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\t\n",
    "\tcbam_feature = Add()([avg_pool,max_pool])\n",
    "\tcbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\t\n",
    "\treturn multiply([input_feature, cbam_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義模型\n",
    "def build_model(num_input_planes=28, k=64, num_int_conv_layers=5): \n",
    "    input_layer = Input(shape=(19, 19, 10))\n",
    "    \n",
    "    x = inception_block(input_layer)\n",
    "    x = Dropout(0.7)(x)\n",
    "    CA = channel_attention(x, 16)\n",
    "    x = Dropout(0.5)(CA)\n",
    "    \n",
    "    x = inception_block(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    CA = channel_attention(x, 16)\n",
    "    x = Dropout(0.5)(CA)\n",
    "    \n",
    "    x = inception_block(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    CA = channel_attention(x, 16)\n",
    "    x = Dropout(0.5)(CA)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    outputs = Dense(3, activation='softmax')(x)\n",
    "            \n",
    "    return Model(inputs=input_layer, outputs=outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer= opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "\n",
    "weight_file = '1128_f10_attention_layerNormal_v3'\n",
    "\n",
    "val_acc='data/GOGO/trainingdata/Play_style/weight/'+ weight_file + '/weights_best_val_acc_{val_accuracy:.4f}.hdf5'\n",
    "val_loss='data/GOGO/trainingdata/Play_style/weight/'+ weight_file + '/weights_best_val_loss_{val_loss:.4f}.hdf5'\n",
    "\n",
    "checkpoint1 = ModelCheckpoint(val_acc, monitor='val_accuracy', verbose=2, save_best_only=True, save_weights_only=False, mode='max')\n",
    "checkpoint2 = ModelCheckpoint(val_loss, monitor='val_loss', verbose=2, save_best_only=True, save_weights_only=False, mode='min')\n",
    "\n",
    "csv_logger = CSVLogger('weight/Play_Style/'+ weight_file + '/training_log.csv', append=True)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint1,checkpoint2, csv_logger] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_training(hist,save_path):\n",
    "    '''\n",
    "    This function take training model and plot history of accuracy and losses with the best epoch in both of them.\n",
    "    '''\n",
    "\n",
    "    # Define needed variables\n",
    "    tr_acc = hist.history['accuracy']\n",
    "    tr_loss = hist.history['loss']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    index_loss = np.argmin(val_loss)\n",
    "    val_lowest = val_loss[index_loss]\n",
    "    index_acc = np.argmax(val_acc)\n",
    "    acc_highest = val_acc[index_acc]\n",
    "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
    "    loss_label = f'best epoch= {str(index_loss + 1)}'\n",
    "    acc_label = f'best epoch= {str(index_acc + 1)}'\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize= (20, 8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
    "    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
    "    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    # plt.ylim(-1,2.5)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
    "    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
    "    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout\n",
    "    plt.savefig('data/GOGO/trainingdata/Play_style/weight/'+ save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "rotated_train_images, rotated_train_labels = shuffle(rotated_train_images, rotated_train_labels, random_state=1123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(rotated_train_images, rotated_train_labels, epochs=100, batch_size=64,validation_split=0.2, verbose=1,callbacks=callbacks_list) #\n",
    "\n",
    "plot_training(history,f'{weight_file}/histrory_1_1.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 驗證模型\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# 輸出結果\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
